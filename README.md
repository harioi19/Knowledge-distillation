# Knowledge-distillation
Knowledge Distillation trains a compact student network to mimic the soft predictions of a large teacher model. By learning from the teacherâ€™s probability distribution (soft targets), the student achieves better generalization and efficiency than training on hard labels alone.
